{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montpellier Open Street Map Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Street Map is a map created using data contributed by volunteers around the world; local mappers and GIS professionals keep the maps up to date by uploading and editing information about different roads and landmarks around the world.\n",
    "\n",
    "I've downloaded the .osm data for the map of the Montpellier area in France. I cleaned the data to fix invalid street, city, and postal code names and reformatted the data to store it in a tabular format **(Section 1)**. After downloading the data into .csv files, I uploaded them into a SQL database to investigate some different attributes of the data **(Section 2)**.\n",
    "\n",
    "Querying the node tags allowed me to understand general features of the city, such as popular cuisine and tree species. **(Section 3)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Cleaning Montpellier Address Data\n",
    "\n",
    "The .osm file for Montpellier is 199,267 KB so I chose to use cElementTree with the iterparse method to work with the .osm data. I created a smaller sample file to use when validating code functionality (SAMPLE_FILE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I've created a dictionary containing all tags in the XML dataset, and the number of each type. These tags refer to elements within the Open Street Map (OSM) data, as well as tags for each element. \n",
    "\n",
    "OSM XML is made up of three different elements: '\n",
    "\n",
    "1. Nodes - defined location made up of an ID and pair of coordinates\n",
    "2. Ways - paths between nodes, ways to get places, linear features, boundaries\n",
    "3. Relations - explains how elements work together, often an ordered list of nodes/ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'count_tags' counts the number of different elements and tags (used to describe features of elements) in the file.   \n",
    "\n",
    "Output: {'bounds': 1,\n",
    " 'member': 19106,\n",
    " 'nd': 1196463,\n",
    " 'node': 854439,\n",
    " 'osm': 1,\n",
    " 'relation': 1610,\n",
    " 'tag': 460554,\n",
    " 'way': 140286}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tag has a key describing the tag attribute, which is held as the 'value'. The function key_type is used to understand how many of the keys have problem characters, or colons (which may be used to nest information).\n",
    "\n",
    "```\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "lower_double_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "```\n",
    "Output: {'lower': 433135,\n",
    " 'lower_colon': 25148,\n",
    " 'lower_double_colon': 335,\n",
    " 'other': 1934,\n",
    " 'problemchars': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Street Names\n",
    "\n",
    "The first cleaning task involved making the street names consistant.\n",
    "\n",
    "The list of 'expected' street names shows the words that I expected to find that describe different streets in Montpellier. By looking through a map of the city, I came up with some expected words such as 'rue' (translates to 'road'), 'avenue', 'boulevard', 'route', 'chemin' (translates to 'path'). In the French language, these words tend to come at the beginning of the road title (e.g. Rue Ferdinand Fabre).\n",
    "\n",
    "For roads that may have been abbreviated, I created the dictionary 'mapping' to update abbreviations to the full name.\n",
    "```\n",
    "mapping = { \"av.\": \"Avenue\",\n",
    "            \"ave\": \"Avenue\",\n",
    "            \"Ave.\":\"Avenue\",\n",
    "            \"R.\":\"Rue\",\n",
    "            \"r.\":\"Rue\",\n",
    "            \"blvd\":\"Boulevard\",\n",
    "            \"blvd.\":\"Boulevard\"\n",
    "            }\n",
    "```\n",
    "The map and list of expected words was used to update street names using the 'update_name' function as data was being put into the SQL database.\n",
    "```\n",
    "def update_name(name, mapping):\n",
    "    updated_name_list=[]\n",
    "    name_list=name.split()\n",
    "    updated_names=name\n",
    "    \n",
    "``` \n",
    "I first checked to see if the road name begins in middle of phrase (e.g. 11, Rue Ferdinand Fabre). For phrases that include the building name or number before the street name, every part of the address before the street name is removed.\n",
    "```\n",
    "    for road in expected:\n",
    "        if road in name_list:\n",
    "            i=0\n",
    "            for name in name_list:\n",
    "                if name==road:\n",
    "                    updated_name_list=name_list[i:]\n",
    "                    updated_names=' '.join(updated_name_list)\n",
    "                    break\n",
    "                else:\n",
    "                    i+=1```\n",
    "                    \n",
    "                    \n",
    "            \n",
    "If this was not the case, I checked if the first word can be mapped to a different word in the correction list for mapping.\n",
    "\n",
    "```\n",
    "\n",
    "    if name_list[0] in mapping.keys():\n",
    "            name_list[0] = mapping[name_list[0]]\n",
    "            updated_names=' '.join(name_list)\n",
    "    return updated_names\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that became clear when looking through the sources of this data, was that the french accents were incoded as a special value. The UCF-8 characters are recorded as ASCII strings. I decided to leave the encoded text, but have accounted for it throughout the cleaning procedure. For example, the word u\"All\\xe9e\", the UCF-8 encoding for allée, was added to the list of expected roads (translating to alley in French)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the same method as above, I investigated the city, country, and postal code of each of these tags.\n",
    "\n",
    "All country codes were properly labelled \"FR\" for France.\n",
    "\n",
    "For the postal codes, I expected to see the four codes attributed to the city of Montpellier: \"34000\",\"34070\",\"34080\",\"34090\"\n",
    "\n",
    "Besides these codes, I list of postal codes for surrounding communes was returned ['34130', '34170', '34006', '34920', '34790', '34970', '34880', '34990', '34830']. This makes sense because in pulling the OSM data for Montpellier, I selected a rectangular region. This would have included communes outside of the city of Montpellier.\n",
    "\n",
    "#### For this reason, I decided to assess the validity of the data by filtering the postal codes only by the département of Hérault and postal code length (5 characters). \n",
    "In French postal codes, the first two numbers refer to the département (34 in this case). All postal codes in this dataset began with the digits '34', but some were longer than 5 characters (e.g. '34064 Montpellier Cedex 2'). For all postal codes greater than 5 characters, I used a function to return only the first 5 characters of the postal code.\n",
    "\n",
    "```def investigate_zip(zip_value,code_list):\n",
    "    new_code=zip_value\n",
    "    if len(zip_value)>5: #check for postcodes with text appended \n",
    "        code_list.add(tag.attrib['v']) #add to list for reference\n",
    "        new_code=new_code[:5]\n",
    "    if zip_value[:2] !='34': #check for non-Herault codes\n",
    "        new_code='error' #entry to be discluded later\n",
    "        code_list.add(tag.attrib['v'])\n",
    "    return new_code\n",
    "    ```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix city data, I looked at the different cities in the dataset, and added valid communes to the list of valid cities. For common typos, or areas within a hamlet, I setup a dictionary that could be used to fix the names or assign the appropriate hamlet.\n",
    "```\n",
    "#create dictionary of things to change faulty street names to\n",
    "    mapping_city = { \"Castelnau le Lez\": \"Castelnau-le-Lez\",\n",
    "                \"Montpelier\": \"Montpellier\",\n",
    "                \"Saint-Jean-de-Vedas\":u'Saint-Jean-de-V\\xe9das',\n",
    "                \"Montpelle\":\"Montpellier\",\n",
    "                'Castelnau le lez':\"Castelnau-le-Lez\",\n",
    "                'Castelnau le Les':\"Castelnau-le-Lez\",\n",
    "                 u'Saint Cl\\xe9ment de riviere':u'Saint-Cl\\xe9ment-de-Rivi\\xe8re',\n",
    "                'Maurin':'Lattes'\n",
    "                }```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Parsing data into SQL Database\n",
    "\n",
    "In order to put this data into an SQL database, I will parse each element in the XML file, putting them into a tabular format that can be written to a .csv file. I'm using a schema and validation library to check the data before writing the data structures to new .csv files.\n",
    "\n",
    "The \"node\" field has a dictionary with the following attributes:\n",
    "- id, user ,uid, version, lat, lon, timestamp, changeset\n",
    "\n",
    "The \"node_tags\" contains a list of dictionaries of different tags for each node containing the following attributes:\n",
    "- id: id of the top level node\n",
    "- key: characters before a colon are removed and added to the 'type' field\n",
    "- value\n",
    "- type:  \"regular\" if a colon is not present.\n",
    "\n",
    "The dictionaries for the 'way' field contain the following attributes. There is also a list of tags formatted in the same form as for the node tags.\n",
    "\n",
    "- id, user, uid, version, timestamp, changeset\n",
    "\n",
    "The \"way_nodes\" holds list of dictionaries, one for each nd child tag.  Each dictionary has the following attributes:\n",
    "- id\n",
    "- node_id\n",
    "- position: index to reference position\n",
    "\n",
    "```         \n",
    "            if PROBLEMCHARS.search(minitag.attrib[\"k\"])==None:\n",
    "                \n",
    "                #first,fix streetname\n",
    "                if minitag.attrib['k'] == \"addr:street\":\n",
    "                    #change road name to fixed name \n",
    "                    minitag.attrib['v']=audit_street_type(street_types, minitag.attrib['v'])```\n",
    "\n",
    "Keys and attributes are then added to the dictionary and split if a colon was present in the key.\n",
    "\n",
    "```           \n",
    "      \n",
    "                dic[\"id\"]=element.attrib[\"id\"]\n",
    "                if \":\" in minitag.attrib[\"k\"]:\n",
    "                    splitkey=minitag.attrib[\"k\"]\n",
    "                    splitkey=splitkey.split(\":\",1)\n",
    "                    dic[\"key\"]=splitkey[1]\n",
    "                    dic[\"type\"]=splitkey[0]\n",
    "                else:\n",
    "                    dic[\"key\"]=minitag.attrib[\"k\"]\n",
    "                    dic[\"type\"]=\"regular\"\n",
    "                dic['value']=minitag.attrib['v']\n",
    "            ```\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the .csv files were written, I imported them into a sql database using sqlite3. I chose to use python to specify the data types and include the program in the same workflow. The processed used to create the database files and tables is shown below. It was repeated for each of the five tables.\n",
    "\n",
    "\n",
    "Final database files:\n",
    "\n",
    "nodes.db : 64,273 KB\n",
    "nodes_tags.db : 4,340 KB\n",
    "ways.db : 16,087 KB\n",
    "ways_tags.db : 45,493 KB\n",
    "ways_nodes.db : 22,903 KB\n",
    "\n",
    "montpellier.db: 122,303 KB\n",
    "\n",
    "During this process, I also counted the number of unique nodes and ways in the dataset. There's a total of 854,439 nodes (see count query below) and 140,286 ways. It makes sense that there are more nodes, as each way is made up of a collection of nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. User Contribution Analysis\n",
    "\n",
    "Below is the code used to count how many users contributed to the Montpellier OSM data. There was a total of 714 entries that had distinct user IDs.\n",
    "\n",
    "```#select user entry data\n",
    "c.execute('SELECT user,COUNT(*) as num FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e GROUP BY e.user ORDER BY num DESC;')\n",
    "all_rows = c.fetchall()```\n",
    "\n",
    "After combining user ID list with the respective number of entries, I looked at some summary statistics for the DataFrame.\n",
    "\n",
    "Attribute|Value\n",
    "---|---\n",
    "mean  |     1393.172269\n",
    "std    |   16322.682447\n",
    "min     |      1.000000\n",
    "25%      |     2.000000\n",
    "50%       |    7.000000\n",
    "75%        |  36.000000\n",
    "max     | 392261.000000\n",
    "\n",
    "The max user contributed 392261 unique nodes/ways, but 75% of users contributed 36 or fewer OSM entries. This leads me to believe that a subset of users are entering data programatically, while the majority of users enter this data manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landmarks in Montpellier\n",
    "\n",
    "I began by running a SQL query to understand the different tags used to describe different nodes, pulling the 300 most popular entries to browse. I looked at the most popular values for tags that interested me.\n",
    "\n",
    "### Food\n",
    "\n",
    "```\n",
    "#investigate tag types\n",
    "c.execute('SELECT key, COUNT(*) FROM node_tags GROUP BY key ORDER BY COUNT(*) DESC LIMIT 10;')\n",
    "```\n",
    "\n",
    "It was interesting to compare the french and american cultures. For example, although kebabs are the 3rd most common cuisine in Montpellier. Traditional kebab restaurants are rarely eaten in the United States. Their popularity stems from the introduction of the kebab Turkish immigrants and popularity with the North African French population (http://www.reuters.com/article/us-france-immigration-kebabs-idUSKBN0IH0CQ20141028).\n",
    "\n",
    "```\n",
    "c.execute('SELECT value, COUNT(*) as num FROM node_tags WHERE key=\"cuisine\" GROUP BY value ORDER BY num DESC LIMIT 10;')\n",
    "all_rows = c.fetchall()\n",
    "pprint(all_rows)\n",
    "print(\"\\n\")\n",
    "```\n",
    "### Shops\n",
    "\n",
    "Bakeries and butchers are among the most common stores in Montpellier. Unlike in many cities in the United States, French locals often choose to buy their bread/pastries and meat at stores separate from a large supermarket.\n",
    "\n",
    "```\n",
    "c.execute('SELECT value, COUNT(*) as num FROM node_tags WHERE key=\"shop\" GROUP BY value ORDER BY num DESC LIMIT 10;')\n",
    "```\n",
    "\n",
    "Output: [(u'clothes', 191),\n",
    " (u'hairdresser', 183),\n",
    " (u'bakery', 164),\n",
    " (u'convenience', 108),\n",
    " (u'butcher', 87),\n",
    " (u'estate_agent', 86),\n",
    " (u'furniture', 85),\n",
    " (u'beauty', 67),\n",
    " (u'newsagent', 63),\n",
    " (u'greengrocer', 59)]\n",
    "\n",
    "### Tree Species\n",
    "\n",
    "I also looked at the most common tree species in Montpellier, learning that 'Platanus x acerifolia', or the London Planetree, is the most common species in the city. These types of trees are popular within cities due to their resistance to warm weather and pollution.\n",
    "\n",
    "```\n",
    "c.execute('SELECT value, COUNT(*) as num FROM node_tags WHERE key=\"species\" GROUP BY value ORDER BY num DESC LIMIT 5;')\n",
    "```\n",
    "Output: [(u'Platanus x acerifolia', 113),\n",
    " (u'pinus pinea', 75),\n",
    " (u'Pin Pignon', 70),\n",
    " (u'Micocoulier de Provence', 32),\n",
    " (u'C. sempervirens', 16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Digital Tour of the City\n",
    "\n",
    "I was interested in understanding what types of trees were in the city and I enjoy learning about the species the grow in different regions of the world. This data could certainly be used to find different species of trees within the city.\n",
    "\n",
    "The tag ID can be used to join with the parent node ID and recover the latitude and longitude of any specific tree. This could be a useful tool for tree enthusiasts who want to know more about what areas of the city the Mediterannean Cyprus grows, and where they could see one in person.\n",
    "\n",
    "Since many OSM contributers find their data using aerial photography and GPS, it's possible that these species were extracted using imprecise methods using only size or color of the tree. Information could be validated and would need to be updated semi-regularly as new trees are planted.\n",
    "\n",
    "A platform could be created for users to walk around the city, being able to see landmarks such as trees or finding different types of cuisne. The GPS data is readily available and can be extracted to find the appropriate GPS coordinates for different landmarks in a city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other Ways to Improve Data Quality\n",
    "\n",
    "One way to improve the validity of the data, as in the case of correcting incorrect postal codes, would be to use an application to 'inspect' certain tag attributes after they've been created. For users generating large amounts of data, a function such as the function 'investigate_zip' could be used to recognize postal codes that do not meet a decided postal code attribute specification.\n",
    "\n",
    "Data could also be inspected for uniqueness. If two nodes have latitudes and longitudes similar to a certain degree, the nodes could be recognized with an inspection function to allow the user to confirm whether the nodes are the same (and one can be removed), different (and the location could be changed), or different objects in very close proximity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
